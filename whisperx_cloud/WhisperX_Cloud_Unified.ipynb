{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperX Cloud API Server ğŸš€ (Universal)\n",
    "\n",
    "**æ”¯æŒå¹³å°:** Google Colab | Kaggle | æœ¬åœ° GPU æœåŠ¡å™¨\n",
    "\n",
    "è¿™ä¸ª Notebook è‡ªåŠ¨æ£€æµ‹è¿è¡Œç¯å¢ƒå¹¶éƒ¨ç½² WhisperX API æœåŠ¡ã€‚\n",
    "\n",
    "## ğŸ¯ ç‰¹æ€§\n",
    "- âœ… è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒ (Colab/Kaggle/Local)\n",
    "- âœ… è‡ªåŠ¨ GPU/CPU æ£€æµ‹\n",
    "- âœ… å†…ç½® ngrok éš§é“ï¼ˆå…¬å…± URLï¼‰\n",
    "- âœ… æ”¯æŒå¤šè¯­è¨€\n",
    "- âœ… å•è¯çº§æ—¶é—´æˆ³å¯¹é½\n",
    "- âœ… å¯é€‰è¯´è¯äººåˆ†ç¦»\n",
    "\n",
    "## ğŸ“‹ ä½¿ç”¨æ­¥éª¤\n",
    "1. **è®¾ç½® GPU**: åœ¨ Colab/Kaggle ä¸­å¯ç”¨ GPU\n",
    "2. **é…ç½® ngrok**: åœ¨ä¸‹é¢è®¾ç½® ngrok token\n",
    "3. **è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼**: ç‚¹å‡» Runtime â†’ Run all\n",
    "4. **å¤åˆ¶ URL**: å°†ç”Ÿæˆçš„å…¬å…± URL å¤åˆ¶åˆ° VideoLingo é…ç½®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0ï¸âƒ£ ç¯å¢ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - åœ¨è¿™é‡Œé…ç½®ä½ çš„å‚æ•°\n",
    "# ============================================\n",
    "\n",
    "# ngrok è®¤è¯ä»¤ç‰Œ (å¿…éœ€ - ä» https://dashboard.ngrok.com è·å–)\n",
    "NGROK_AUTH_TOKEN = \"\"\n",
    "\n",
    "# API æœåŠ¡ç«¯å£\n",
    "SERVER_PORT = 8000\n",
    "\n",
    "# é»˜è®¤ Whisper æ¨¡å‹\n",
    "DEFAULT_MODEL = \"large-v3\"\n",
    "\n",
    "# æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦» (éœ€è¦æ›´å¤š GPU å†…å­˜)\n",
    "ENABLE_DIARIZATION = False\n",
    "\n",
    "# HuggingFace é•œåƒ (ä¸­å›½å¤§é™†ç”¨æˆ·å¯è®¾ç½®ä¸º \"https://hf-mirror.com\")\n",
    "HF_ENDPOINT = \"https://huggingface.co\"\n",
    "\n",
    "print(\"âš™ï¸ Configuration loaded\")\n",
    "print(f\"   Model: {DEFAULT_MODEL}\")\n",
    "print(f\"   Port: {SERVER_PORT}\")\n",
    "print(f\"   HF Endpoint: {HF_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒæ£€æµ‹ä¸è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ£€æµ‹è¿è¡Œç¯å¢ƒ\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# æ£€æµ‹å¹³å°\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "IN_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "print(\"ğŸ” Environment Detection:\")\n",
    "print(f\"   Google Colab: {IN_COLAB}\")\n",
    "print(f\"   Kaggle: {IN_KAGGLE}\")\n",
    "print(f\"   Local: {IN_LOCAL}\")\n",
    "\n",
    "# è®¾ç½® HuggingFace ç«¯ç‚¹\n",
    "os.environ['HF_ENDPOINT'] = HF_ENDPOINT\n",
    "\n",
    "# Kaggle ç‰¹æ®Šå¤„ç†\n",
    "if IN_KAGGLE:\n",
    "    print(\"\\nğŸ“Œ Kaggle Instructions:\")\n",
    "    print(\"   1. Settings â†’ Accelerator â†’ GPU T4 x2\")\n",
    "    print(\"   2. Internet must be ON for ngrok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ GPU æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ£€æŸ¥ GPU å¯ç”¨æ€§\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\nâš ï¸  WARNING: No GPU detected or nvidia-smi not found\")\n",
    "    print(\"   Server will run in CPU mode (slow)\\n\")\n",
    "else:\n",
    "    print(\"\\nâœ… GPU detected!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å®‰è£…ä¾èµ–åŒ…\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ“¦ Installing dependencies...\\n\")\n",
    "\n",
    "# æ ¸å¿ƒä¾èµ–\n",
    "packages = [\n",
    "    \"fastapi\",\n",
    "    \"uvicorn[standard]\",\n",
    "    \"python-multipart\",\n",
    "    \"pydantic\",\n",
    "    \"requests\",\n",
    "    \"pyngrok\",\n",
    "]\n",
    "\n",
    "# Kaggle éœ€è¦ nest_asyncio\n",
    "if IN_KAGGLE:\n",
    "    packages.append(\"nest_asyncio\")\n",
    "\n",
    "# å®‰è£…\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    print(f\"   âœ… {pkg}\")\n",
    "\n",
    "# å®‰è£… PyTorch (CUDA 11.8)\n",
    "print(\"\\nğŸ“¦ Installing PyTorch with CUDA support...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"torch==2.0.0+cu118\", \"torchaudio==2.0.0+cu118\",\n",
    "    \"--extra-index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
    "])\n",
    "print(\"   âœ… PyTorch\")\n",
    "\n",
    "# å®‰è£… WhisperX\n",
    "print(\"\\nğŸ“¦ Installing WhisperX...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"whisperx@git+https://github.com/m-bain/whisperx.git@7307306a9d8dd0d261e588cc933322454f853853\"\n",
    "])\n",
    "print(\"   âœ… WhisperX\")\n",
    "\n",
    "# å¯é€‰ï¼šè¯´è¯äººåˆ†ç¦»\n",
    "if ENABLE_DIARIZATION:\n",
    "    print(\"\\nğŸ“¦ Installing speaker diarization...\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"pyannote.audio==3.1.1\"\n",
    "    ])\n",
    "    print(\"   âœ… pyannote.audio\")\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ åˆ›å»º API æœåŠ¡å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ç”ŸæˆæœåŠ¡å™¨ä»£ç \n",
    "# ============================================\n",
    "\n",
    "server_code = '''\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "import time\n",
    "import platform\n",
    "from typing import Optional\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Environment-specific imports\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import whisperx\n",
    "import librosa\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global state\n",
    "model_cache = {}\n",
    "device = None\n",
    "compute_type = None\n",
    "platform_name = None\n",
    "\n",
    "class TranscriptionResponse(BaseModel):\n",
    "    success: bool\n",
    "    language: str\n",
    "    segments: list\n",
    "    word_segments: Optional[list] = None\n",
    "    speakers: Optional[list] = None\n",
    "    processing_time: float\n",
    "    device: str\n",
    "    model: str\n",
    "    platform: str\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global device, compute_type, platform_name\n",
    "    \n",
    "    # Detect platform\n",
    "    if os.path.exists('/kaggle'):\n",
    "        platform_name = \"Kaggle\"\n",
    "    elif 'google.colab' in str(globals()):\n",
    "        platform_name = \"Colab\"\n",
    "    else:\n",
    "        platform_name = \"Local\"\n",
    "    \n",
    "    # Detect device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cuda\":\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        compute_type = \"float16\" if torch.cuda.is_bf16_supported() else \"int8\"\n",
    "        print(f\"ğŸš€ Platform: {platform_name}\")\n",
    "        print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ğŸ’¾ GPU Memory: {gpu_mem:.2f} GB\")\n",
    "        print(f\"âš™ï¸  Compute type: {compute_type}\")\n",
    "    else:\n",
    "        compute_type = \"int8\"\n",
    "        print(f\"âš ï¸  CPU mode on {platform_name}\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup\n",
    "    print(\"\\nğŸ§¹ Cleaning up...\")\n",
    "    model_cache.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"WhisperX Cloud API\",\n",
    "    description=\"Universal WhisperX ASR service for VideoLingo\",\n",
    "    version=\"2.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "def get_or_load_model(model_name: str, language: Optional[str] = None):\n",
    "    \"\"\"Load or get cached model\"\"\"\n",
    "    cache_key = f\"{model_name}_{language}_{compute_type}\"\n",
    "    \n",
    "    if cache_key not in model_cache:\n",
    "        print(f\"\\nğŸ“¥ Loading model: {model_name}...\")\n",
    "        \n",
    "        vad_options = {\"vad_onset\": 0.500, \"vad_offset\": 0.363}\n",
    "        asr_options = {\"temperatures\": [0], \"initial_prompt\": \"\"}\n",
    "        \n",
    "        model = whisperx.load_model(\n",
    "            model_name,\n",
    "            device,\n",
    "            compute_type=compute_type,\n",
    "            language=language,\n",
    "            vad_options=vad_options,\n",
    "            asr_options=asr_options\n",
    "        )\n",
    "        model_cache[cache_key] = model\n",
    "        print(f\"âœ… Model loaded: {model_name}\")\n",
    "    \n",
    "    return model_cache[cache_key]\n",
    "\n",
    "def process_audio(audio_bytes: bytes):\n",
    "    \"\"\"Process uploaded audio\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
    "        tmp.write(audio_bytes)\n",
    "        tmp_path = tmp.name\n",
    "    \n",
    "    try:\n",
    "        audio, sr = librosa.load(tmp_path, sr=16000, mono=True)\n",
    "        return audio\n",
    "    finally:\n",
    "        os.unlink(tmp_path)\n",
    "\n",
    "# ==================== API Endpoints ====================\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    gpu_mem = None\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"platform\": platform_name,\n",
    "        \"device\": device,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"gpu_memory_gb\": gpu_mem,\n",
    "        \"models_cached\": len(model_cache),\n",
    "        \"model_keys\": list(model_cache.keys())\n",
    "    }\n",
    "\n",
    "@app.post(\"/transcribe\", response_model=TranscriptionResponse)\n",
    "async def transcribe(\n",
    "    audio: UploadFile = File(..., description=\"Audio file to transcribe\"),\n",
    "    language: Optional[str] = Form(None, description=\"Language code (e.g., en, zh, ja)\"),\n",
    "    model: str = Form(\"large-v3\", description=\"Whisper model name\"),\n",
    "    batch_size: Optional[int] = Form(None, description=\"Batch size (auto if not set)\"),\n",
    "    align: bool = Form(True, description=\"Enable word-level alignment\"),\n",
    "    speaker_diarization: bool = Form(False, description=\"Enable speaker diarization\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Transcribe audio file\n",
    "    \n",
    "    Returns segments with word-level timestamps\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Auto batch size\n",
    "    if batch_size is None:\n",
    "        if device == \"cuda\":\n",
    "            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            batch_size = 16 if gpu_mem > 8 else 4\n",
    "        else:\n",
    "            batch_size = 1\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nğŸ¯ Transcribing: {audio.filename}\")\n",
    "        print(f\"   Language: {language or 'auto-detect'}\")\n",
    "        print(f\"   Model: {model}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        \n",
    "        # Read and process audio\n",
    "        audio_bytes = await audio.read()\n",
    "        audio_array = process_audio(audio_bytes)\n",
    "        \n",
    "        # Load model and transcribe\n",
    "        whisper_model = get_or_load_model(model, language)\n",
    "        result = whisper_model.transcribe(audio_array, batch_size=batch_size)\n",
    "        \n",
    "        detected_language = result.get(\"language\", language or \"unknown\")\n",
    "        segments = result.get(\"segments\", [])\n",
    "        \n",
    "        print(f\"\\nâœ… Transcription done: {len(segments)} segments\")\n",
    "        \n",
    "        # Word-level alignment\n",
    "        word_segments = None\n",
    "        if align and segments:\n",
    "            print(\"ğŸ”„ Aligning words...\")\n",
    "            align_model, align_metadata = whisperx.load_align_model(\n",
    "                language_code=detected_language,\n",
    "                device=device\n",
    "            )\n",
    "            result_aligned = whisperx.align(\n",
    "                segments,\n",
    "                align_model,\n",
    "                align_metadata,\n",
    "                audio_array,\n",
    "                device,\n",
    "                return_char_alignments=False\n",
    "            )\n",
    "            segments = result_aligned.get(\"segments\", [])\n",
    "            word_segments = result_aligned.get(\"word_segments\", [])\n",
    "            del align_model\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"âœ… Alignment done: {len(word_segments or [])} words\")\n",
    "        \n",
    "        # Speaker diarization\n",
    "        speakers = None\n",
    "        if speaker_diarization:\n",
    "            print(\"ğŸ­ Performing speaker diarization...\")\n",
    "            try:\n",
    "                diarize_model = whisperx.DiarizationPipeline(\n",
    "                    model_name=\"pyannote/speaker-diarization-3.1\",\n",
    "                    device=device\n",
    "                )\n",
    "                diarize_segments = diarize_model(audio_array)\n",
    "                result_diarized = whisperx.assign_word_speakers(diarize_segments, {\"segments\": segments})\n",
    "                segments = result_diarized.get(\"segments\", [])\n",
    "                speakers = list(set(seg.get(\"speaker\", \"UNKNOWN\") for seg in segments if \"speaker\" in seg))\n",
    "                print(f\"âœ… Diarization done: {len(speakers)} speakers\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Diarization failed: {e}\")\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return TranscriptionResponse(\n",
    "            success=True,\n",
    "            language=detected_language,\n",
    "            segments=segments,\n",
    "            word_segments=word_segments,\n",
    "            speakers=speakers,\n",
    "            processing_time=processing_time,\n",
    "            device=device,\n",
    "            model=model,\n",
    "            platform=platform_name\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\nâŒ Error: {error_msg}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise HTTPException(status_code=500, detail=error_msg)\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "@app.delete(\"/cache\")\n",
    "async def clear_cache():\n",
    "    \"\"\"Clear model cache to free GPU memory\"\"\"\n",
    "    global model_cache\n",
    "    cleared_count = len(model_cache)\n",
    "    model_cache.clear()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return {\n",
    "        \"status\": \"Cache cleared\",\n",
    "        \"models_cleared\": cleared_count\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def get_stats():\n",
    "    \"\"\"Get server statistics\"\"\"\n",
    "    gpu_stats = {}\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_stats = {\n",
    "            \"name\": torch.cuda.get_device_name(0),\n",
    "            \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / (1024**3),\n",
    "            \"allocated_memory_gb\": torch.cuda.memory_allocated() / (1024**3),\n",
    "            \"reserved_memory_gb\": torch.cuda.memory_reserved() / (1024**3),\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"platform\": platform_name,\n",
    "        \"device\": device,\n",
    "        \"models_cached\": len(model_cache),\n",
    "        \"gpu\": gpu_stats\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    port = int(os.environ.get(\"PORT\", 8000))\n",
    "    host = os.environ.get(\"HOST\", \"0.0.0.0\")\n",
    "    uvicorn.run(app, host=host, port=port)\n",
    "'''\n",
    "\n",
    "# å†™å…¥æ–‡ä»¶\n",
    "with open('whisperx_server.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(server_code)\n",
    "\n",
    "print(\"âœ… Server code created: whisperx_server.py\")\n",
    "print(f\"   Size: {len(server_code)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ é…ç½® ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# é…ç½® ngrok\n",
    "# ============================================\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "if not NGROK_AUTH_TOKEN:\n",
    "    print(\"âŒ ERROR: NGROK_AUTH_TOKEN is not set!\")\n",
    "    print(\"\\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è·å–:\")\n",
    "    print(\"1. è®¿é—® https://dashboard.ngrok.com/signup\")\n",
    "    print(\"2. æ³¨å†Œå¹¶ç™»å½•\")\n",
    "    print(\"3. è®¿é—® https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"4. å¤åˆ¶ token åˆ°ä¸Šé¢çš„é…ç½®å•å…ƒæ ¼\")\n",
    "    raise ValueError(\"NGROK_AUTH_TOKEN required\")\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "print(\"âœ… ngrok configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ å¯åŠ¨æœåŠ¡å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å¯åŠ¨ API æœåŠ¡å™¨å’Œ ngrok éš§é“\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "\n",
    "# æ¸…ç†æ—§è¿›ç¨‹\n",
    "print(\"ğŸ§¹ Cleaning up old processes...\")\n",
    "subprocess.run(\"pkill -f whisperx_server.py 2>/dev/null || true\", shell=True)\n",
    "subprocess.run(\"pkill -f ngrok 2>/dev/null || true\", shell=True)\n",
    "time.sleep(2)\n",
    "\n",
    "# å…³é—­ç°æœ‰ ngrok éš§é“\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ['PORT'] = str(SERVER_PORT)\n",
    "os.environ['HF_ENDPOINT'] = HF_ENDPOINT\n",
    "\n",
    "# å¯åŠ¨æœåŠ¡å™¨\n",
    "print(\"\\nğŸš€ Starting WhisperX API server...\")\n",
    "server_process = subprocess.Popen(\n",
    "    [sys.executable, 'whisperx_server.py'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    preexec_fn=os.setsid if hasattr(os, 'setsid') else None\n",
    ")\n",
    "\n",
    "# ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨\n",
    "print(\"â³ Waiting for server to start (10s)...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# å¯åŠ¨ ngrok\n",
    "print(\"\\nğŸŒ Creating ngrok tunnel...\")\n",
    "try:\n",
    "    public_url = ngrok.connect(SERVER_PORT, \"http\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… SERVER IS RUNNING!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸŒ Public URL: {public_url}\")\n",
    "    print(f\"ğŸ”— API Endpoint: {public_url}/transcribe\")\n",
    "    print(f\"ğŸ¥ Health Check: {public_url}/\")\n",
    "    print(f\"ğŸ“Š Stats: {public_url}/stats\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ Copy the Public URL to VideoLingo config!\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ä¿å­˜ URL\n",
    "    with open('server_url.txt', 'w') as f:\n",
    "        f.write(str(public_url))\n",
    "    \n",
    "    SERVER_URL = str(public_url)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error starting ngrok: {e}\")\n",
    "    print(\"\\nTrying local tunnel alternative...\")\n",
    "    SERVER_URL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ æµ‹è¯• API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æµ‹è¯• API è¿æ¥\n",
    "# ============================================\n",
    "\n",
    "import requests\n",
    "\n",
    "# è¯»å– URL\n",
    "try:\n",
    "    with open('server_url.txt', 'r') as f:\n",
    "        API_URL = f.read().strip()\n",
    "except:\n",
    "    API_URL = f\"http://localhost:{SERVER_PORT}\"\n",
    "\n",
    "print(f\"Testing: {API_URL}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# å¥åº·æ£€æŸ¥\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/\", timeout=10)\n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"âœ… Health Check PASSED\\n\")\n",
    "    print(f\"   Status: {data.get('status')}\")\n",
    "    print(f\"   Platform: {data.get('platform')}\")\n",
    "    print(f\"   Device: {data.get('device')}\")\n",
    "    if data.get('gpu_memory_gb'):\n",
    "        print(f\"   GPU Memory: {data['gpu_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Models Cached: {data.get('models_cached', 0)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Health check failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is running (previous cell)\")\n",
    "    print(\"2. Try waiting a bit longer and re-run\")\n",
    "    print(\"3. Check ngrok status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ ä¿æŒè¿è¡Œ (âš ï¸ ä¸è¦åœæ­¢æ­¤å•å…ƒæ ¼!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ä¿æŒæœåŠ¡å™¨è¿è¡Œ\n",
    "# ============================================\n",
    "\n",
    "print(\"ğŸ’“ Server is running...\\n\")\n",
    "print(\"Press STOP button in toolbar to stop\\n\")\n",
    "\n",
    "try:\n",
    "    count = 0\n",
    "    while True:\n",
    "        time.sleep(30)\n",
    "        count += 1\n",
    "        \n",
    "        # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å¥åº·çŠ¶æ€\n",
    "        if count % 2 == 0:\n",
    "            try:\n",
    "                r = requests.get(f\"{API_URL}/\", timeout=5)\n",
    "                if r.status_code == 200:\n",
    "                    print(f\"âœ… {time.strftime('%H:%M:%S')} - Server healthy\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸  {time.strftime('%H:%M:%S')} - Status: {r.status_code}\")\n",
    "            except:\n",
    "                print(f\"âš ï¸  {time.strftime('%H:%M:%S')} - Health check failed\")\n",
    "        else:\n",
    "            print(f\"ğŸ’“ {time.strftime('%H:%M:%S')} - Running...\")\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nğŸ›‘ Stopping server...\")\n",
    "    ngrok.kill()\n",
    "    if server_process:\n",
    "        server_process.terminate()\n",
    "        server_process.wait()\n",
    "    print(\"âœ… Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š VideoLingo é…ç½®æŒ‡å—\n",
    "\n",
    "æœåŠ¡å™¨è¿è¡Œåï¼Œå°† URL é…ç½®åˆ° VideoLingo:\n",
    "\n",
    "### æ–¹æ³• 1: ä¿®æ”¹ config.yaml\n",
    "```yaml\n",
    "whisper:\n",
    "  runtime: 'cloud'\n",
    "  whisperX_cloud_url: 'https://xxxx.ngrok-free.app'  # ä½ çš„ ngrok URL\n",
    "```\n",
    "\n",
    "### æ–¹æ³• 2: ç¯å¢ƒå˜é‡\n",
    "```bash\n",
    "export WHISPERX_CLOUD_URL='https://xxxx.ngrok-free.app'\n",
    "```\n",
    "\n",
    "### æ–¹æ³• 3: VideoLingo å®¢æˆ·ç«¯\n",
    "ä½¿ç”¨ `whisperx_cloud_client.py` ä¸­çš„ `WhisperXCloudClient` ç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ•…éšœæ’é™¤\n",
    "\n",
    "### 1. ngrok è¿æ¥å¤±è´¥\n",
    "- æ£€æŸ¥ token æ˜¯å¦æ­£ç¡®\n",
    "- ç¡®è®¤ç½‘ç»œè¿æ¥ï¼ˆKaggle éœ€è¦ Internet ONï¼‰\n",
    "- å°è¯•é‡æ–°è¿è¡Œç¬¬ 5ã€6 å•å…ƒæ ¼\n",
    "\n",
    "### 2. GPU æœªæ£€æµ‹åˆ°\n",
    "- Colab: Runtime â†’ Change runtime type â†’ GPU\n",
    "- Kaggle: Settings â†’ Accelerator â†’ GPU T4 x2\n",
    "\n",
    "### 3. æ¨¡å‹ä¸‹è½½æ…¢\n",
    "- è®¾ç½® HF_ENDPOINT ä¸ºé•œåƒç«™ï¼ˆä¸­å›½å¤§é™†ï¼‰\n",
    "- æˆ–ç­‰å¾…é¦–æ¬¡ä¸‹è½½å®Œæˆï¼ˆçº¦ 3GBï¼‰\n",
    "\n",
    "### 4. æ˜¾å­˜ä¸è¶³\n",
    "- ä½¿ç”¨è¾ƒå°æ¨¡å‹: 'medium' æˆ– 'small'\n",
    "- å‡å° batch_size\n",
    "- ç¦ç”¨ speaker_diarization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
